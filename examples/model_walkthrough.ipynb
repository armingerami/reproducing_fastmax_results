{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Transformer model components:](#Transformer-model-components:)  \n",
    "   1.1 [FMM-Attention factorization](#FMM-Attention-factorization)  \n",
    "   1.2 [Attention]()  \n",
    "   1.3 [Multi-headed Attention]()  \n",
    "   1.4 [Transformer Block]()  \n",
    "   1.5 [Modality-specific components]()  \n",
    "   1.6 [Transformer]()  \n",
    "2. [Training](#training)  \n",
    "3. [Data Prep]()  \n",
    "4. [Running]()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model components:\n",
    "## FMM-Attention factorization\n",
    "\n",
    "Input: Query, Key, and Value matrices (which are n by d matrices), along with n and d\n",
    "Output: The result of Attention matrix * Value (an n by d matrix)\n",
    "The Attention matrix in here is an estimate of softmax. This code is not optimized through Factorization, its just a test to help us take next steps\n",
    "If the result is bad, but not too bad, increas p. It can go up to 4, but from 5 onward it would not be worth it in my opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See score_function.py\n",
    "def score_before_jit(Q, K, V, n, d, p=2):\n",
    "    sqrtd = math.sqrt(d)\n",
    "    fastmax = torch.zeros([n,n])\n",
    "    div = torch.zeros(n)\n",
    "    ans = torch.zeros([n,d])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(p):\n",
    "                fastmax[i][j] += torch.dot(Q[i], K[j])/sqrtd**k/math.factorial(k)\n",
    "            \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            div[i] += fastmax[i][j]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            fastmax[i][j] /= div[i]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(d):\n",
    "            ans[i,j] = torch.dot(fastmax[i], V[:, j])\n",
    "\n",
    "    return ans\n",
    "\n",
    "score = torch.jit.script(score_before_jit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Module\n",
    "\n",
    "The core attention operation of a transformer. The work here is in learning\n",
    "a square n x n attention matrix, A. Each row of A corresponds to a single\n",
    "token and the row is a weighting of how much information should be accumulated\n",
    "from each other token.\n",
    "\n",
    "This weighting is ultimately determined by learning a good linear transformation\n",
    "that maps each token vector to a \"query\" vector. The query vector for each token\n",
    "gets compared with each other token by a \"compatability function\", and higher\n",
    "compatability results in a higher weight in the resulting attention row.\n",
    "\n",
    "Here we use \"dot product attention\" for the compatability function. We could\n",
    "compute the compatability/closeness of the query vector directly with the\n",
    "token vector, but it turns out that learning another linear transformation\n",
    "from the token vector to a \"key\" vector space and then computing the\n",
    "compatability between those produces better results.\n",
    "\n",
    "Finally, we do a mat-mul of the attention matrix with the original matrix of\n",
    "token vectors to get the updates from these weightings. And just as how it\n",
    "is helpful to learn a transformation from the raw token vectors to key vectors,\n",
    "it turns out to also be helpful to learn a transformation to a \"value\" vector\n",
    "space and do the mat-mul update on this space instead. The final output is the\n",
    "updated token vectors in the value vector space.\n",
    "\n",
    "Note on masking/causal attention/auto-regressive/encoder-decoder:  \n",
    "More often than not, we will apply a mask on the attention matrix so that each token only gets weightings from the tokens earlier in the sequence, and the weightings from tokens later in the sequence get zeroed out. A transformer that doesn't apply masking is often referred to as an \"encoder-only\" model (e.g. BERT) and a transformer that applies masking is \"decoder-only\" (e.g. GPT3). Because masking makes it so that tokens can only be updated by information from earlier tokens, masked-attention is sometimes called \"causal attention\", and a model with this forumlation is called \"auto-regressive\".\n",
    "\n",
    "Classic Q, K, V explanation:  \n",
    "Q: what a token \"wants\"   \n",
    "K: what a token \"is\"  \n",
    "V: what a token \"shares\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See model.py\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"The core attention operation of a transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_head, dropout_rate=0.2, use_masking=False, fastmax=False, fmm_p=2):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_masking = use_masking\n",
    "        self.fastmax = fastmax\n",
    "        self.fmm_p = fmm_p\n",
    "        self.query_transform = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.key_transform = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.value_transform = nn.Linear(d_model, d_head, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape: (b, n, d_model)\n",
    "        # b: batch size\n",
    "        # n: num tokens/features\n",
    "        # d_model: dim of vector representation of each token\n",
    "        Q = self.query_transform(X)  # (b, n, d_head)\n",
    "        K = self.key_transform(X)  # (b, n, d_head)\n",
    "        V = self.value_transform(X)  # (b, n, d_head)\n",
    "        if self.fastmax:\n",
    "            n = X.shape[1]\n",
    "            results = []\n",
    "            for i in range(Q.shape[0]):\n",
    "                QQ = Q[i]  # (n, d_head)\n",
    "                KK = K[i]  # (n, d_head)\n",
    "                VV = V[i]  # (n, d_head)\n",
    "                VV_hat = score(QQ, KK, VV, n, self.d_head, self.fmm_p)  # (n, d_head)\n",
    "                results.append(VV_hat)  # (n, d_head)\n",
    "            V_hat = torch.stack(results)  # (b, n, d_head)\n",
    "        elif hasattr(torch.nn.functional, \"scaled_dot_product_attention\"):\n",
    "            # If using PyTorch 2.0+, we have access to built-in attention that implements Flash Attention CUDA kernels\n",
    "            V_hat = F.scaled_dot_product_attention(\n",
    "                Q, K, V,\n",
    "                dropout_p=self.dropout_rate if self.training else 0,\n",
    "                is_causal=self.use_masking,\n",
    "            )\n",
    "        else:\n",
    "            # manual implementation of scaled dot product attention\n",
    "            A = Q @ K.mT  # (b, n, d_head) @ (b, d_head, n) -> (b, n, n)\n",
    "            A = A / math.sqrt(self.d_head)\n",
    "            if self.use_masking:\n",
    "                # Mask out upper triangular of A so that weightings only apply to\n",
    "                # previous tokens in the sequence. Filling with -inf before softmax\n",
    "                # will cause softmax to give weight 0 to these tokens.\n",
    "                n = X.shape[1]\n",
    "                upper_tri_mask = torch.triu(\n",
    "                    torch.ones((n, n), dtype=bool, device=X.device), diagonal=1\n",
    "                )  # Upper triangular matrix\n",
    "                A = A.masked_fill(\n",
    "                    upper_tri_mask, float(\"-inf\")\n",
    "                )  # Fill upper triangular with -inf\n",
    "            A = F.softmax(A, dim=-1)\n",
    "            A = self.dropout(A)\n",
    "            V_hat = A @ V  # (b, n, n) @ (b, n, d_head) -> (b, n, d_head)\n",
    "        return V_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "\n",
    "A simple module runs some number of attention modules in parallel\n",
    "and concatenates the resulting vectors into a single, longer output vector.\n",
    "\n",
    "Each parallel attention head works with vectors of size d_model / num_heads.\n",
    "\n",
    "There is also a simple linear transformation after the concatenation.\n",
    "I'd be interested in running some ablations to see if this is necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Runs multiple attention modules in parallel and concatenates the results.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, d_model, dropout_rate=0.2, use_masking=False, fastmax=False, fmm_p=2):\n",
    "        super().__init__()\n",
    "        d_head = d_model // num_heads\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(AttentionHead(d_model, d_head, dropout_rate, use_masking, fastmax, fmm_p))\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape: (b, n, d_model)\n",
    "        heads_outputs = []\n",
    "        for head in self.heads:\n",
    "            heads_outputs.append(head(X))  # (b, n, d_head)\n",
    "        outputs_concated = torch.cat(heads_outputs, dim=-1)  # (b, n, d_model)\n",
    "        output = self.linear(outputs_concated)  # (b, n, d_model)\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "The basic element of a transformer, which is repeated some number of times\n",
    "in the larger model. Each of these blocks does a round of attention\n",
    "followed by an MLP with one hidden layer. The\n",
    "attention in each block is split into multiple \"heads\" which allow for\n",
    "different attention queries in parallel at each step.\n",
    "\n",
    "In the MPP sub-block, there are no connections between tokens/features -\n",
    "these are just functions mapping each token vector to an updated vector of\n",
    "the same dimensions.\n",
    "\n",
    "Following best practices for training deep networks, layer norm is included\n",
    "after both the attention sub-block and the MLP sub-block. A\n",
    "residual connection is included at each of these points as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Basic element of a Transformer: Attention + MLP.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, d_model, d_mlp, dropout_rate=0.2, use_masking=False, fastmax=False, fmm_p=2):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.attention_subblock = MultiHeadAttention(\n",
    "            num_heads, d_model, dropout_rate, use_masking, fastmax, fmm_p\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.mlp_subblock = nn.Sequential(\n",
    "            nn.Linear(d_model, d_mlp),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_mlp, d_model),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape: (b, n, d_model)\n",
    "        attention_output = self.layer_norm1(X)\n",
    "        attention_output = self.attention_subblock(attention_output)  # (b, n, d_model)\n",
    "        attention_output = attention_output + X  # residual connection\n",
    "        mlp_output = self.layer_norm2(attention_output)\n",
    "        mlp_output = self.mlp_subblock(mlp_output)  # (b, n, d_model)\n",
    "        mlp_output = mlp_output + X  # residual connection\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modality-specific embeddings and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See modules.py\n",
    "class PerTokenClassifier(nn.Module):\n",
    "    \"\"\"Linear classifier that outputs one class prediction per token.\"\"\"\n",
    "    def __init__(self, d_model, n_classes):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.linear = nn.Linear(d_model, n_classes)\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.layer_norm(X)  # (b, n, d_model)\n",
    "        logits = self.linear(X)  # (b, n, n_classes)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class AvgPoolClassifier(nn.Module):\n",
    "    \"\"\"Linear classifier that outputs one class prediction by averaging tokens.\"\"\"\n",
    "    def __init__(self, d_model, n_classes):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = nn.Linear(d_model, n_classes)\n",
    "    def forward(self, X):\n",
    "        X = self.layer_norm(X)  # (b, n, d_model)\n",
    "        X = X.permute(0, 2, 1)  # (b, d_model, n) to fit AdaptiveMaxPool1d input format\n",
    "        X = self.pool(X).squeeze(-1)  # (b, d_model)\n",
    "        logits = self.linear(X)  # (b, n_classes)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class ConvEmbedding(nn.Module):\n",
    "    \"\"\"Embedding for images where each token is a patch of pixels convolved, n_kernels=d_model.\"\"\"\n",
    "    def __init__(self, channels, patch_shape, d_model):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            channels, d_model, kernel_size=patch_shape, stride=patch_shape\n",
    "        )\n",
    "    def forward(self, images):\n",
    "        # images shape: (b, c, h, w)\n",
    "        X = self.conv(images)  # (b, d_model, h/patch_h, w/patch_w);\n",
    "        X = X.flatten(2)  # (b, d_model, n)\n",
    "        return X.transpose(1,2)  # (b, n, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "Simple Transformer model based on the paper \"Attention is all you need\".\n",
    "Consists of some number of repeated, identical blocks. Each block does a\n",
    "round of attention followed by an MLP with one\n",
    "hidden layer. The attention in each block is split into multiple \"heads\"\n",
    "which allow for different attention queries in parallel at each step.\n",
    "\n",
    "There is a simple linear embedding layer at the beginning that maps from\n",
    "the discrete vocabulary of the tokens/features to d_model-dimension vectors of real\n",
    "numbers. A positional encoding is included to explicitly add spatial\n",
    "information to the embedding vector.\n",
    "\n",
    "Finally there is a classification layer that maps from d_model\n",
    "vectors to whatever is needed for the task at hand (e.g. a logit vector\n",
    "for classification over a huge set of vocab_size possible classes).\n",
    "\n",
    "A single transformer model is called an \"decoder\" if it uses a mask in\n",
    "training to only learn attention connections from earlier tokens to later\n",
    "tokens. An \"encoder\" omits this mask and learns the fullest possible\n",
    "contextual representation of every token in the input. The decoder\n",
    "restriction is useful for learning to make predictions on input sequences\n",
    "shorter than the n_tokens the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See model.py\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple model consisting of repeated blocks of Attention + MLP.\n",
    "    in_feature_dim: vector length of each individual token or feature. If using\n",
    "        discrete features such as words, this will be vocab_size because each\n",
    "        word can be represented an integer or by a one-hot vector of that length.\n",
    "        If using image patches, simply the number of pixels in each flattened\n",
    "        patch.\n",
    "    out_dim: again, the desired vector length. If classification, just\n",
    "        the number of classes. If word prediction, the vocab_size so we can\n",
    "        output a probability over each word in the vocab.\n",
    "    max_features: the maximum number of features/tokens/patches in the input. \n",
    "        This is only used to define a positional encoding that learns to \n",
    "        represent spatial positions up to that number. Can be set to a longer\n",
    "        lenth than the actual intended number of tokens in the input.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        out_dim,\n",
    "        max_features,\n",
    "        d_model=512,\n",
    "        d_mlp=2048,\n",
    "        heads_per_block=8,\n",
    "        num_blocks=6,\n",
    "        dropout_rate=0.2,\n",
    "        use_masking=False,\n",
    "        embedding=\"linear\",\n",
    "        classifier=\"per_token\",\n",
    "        vocab_size=None,\n",
    "        patch_shape=None,\n",
    "        patch_channels=None,\n",
    "        fastmax=False,\n",
    "        fmm_p=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers: feature space -> d_model space (X)\n",
    "        if embedding == \"discrete_set\":\n",
    "            self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        elif embedding == \"patch_conv\":\n",
    "            self.token_embedding = ConvEmbedding(patch_channels, patch_shape, d_model)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding type: {embedding}\")\n",
    "        self.pos_embedding = nn.Embedding(max_features, d_model)\n",
    "        \n",
    "        # Transformer core: X -> X_hat\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(\n",
    "                TransformerBlock(\n",
    "                    heads_per_block, d_model, d_mlp, dropout_rate, use_masking, fastmax, fmm_p\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Classifier head: X_hat -> logits\n",
    "        if classifier == \"per_token\":\n",
    "            self.classifier = PerTokenClassifier(d_model, out_dim)\n",
    "        elif classifier == \"avg_pool\":\n",
    "            self.classifier = AvgPoolClassifier(d_model, out_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown classifier type: {classifier}\")\n",
    "        \n",
    "        # Weight initialization\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens shape: (b, n, d_feature) or (b, n) if discrete vocabulary\n",
    "        # b: batch size\n",
    "        # n: num tokens/patches\n",
    "        # d_feature: dim of each token/patch prior to embedding. Accepts integers for discrete feature sets.\n",
    "        # d_model: dim of vector representation of each token\n",
    "        tok_emb = self.token_embedding(tokens)  # (b, n, d_model)\n",
    "        position_indices = torch.arange(tokens.shape[1], device=tokens.device)  # (n,)\n",
    "        pos_emb = self.pos_embedding(position_indices)  # (n, d_model)\n",
    "        X = tok_emb + pos_emb  # (b, n, d_model)\n",
    "        X = self.blocks(X)  # (b, n, d_model)\n",
    "        logits = self.classifier(X)  # (b, n, out_dim)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Use the Pytorch Lightning library to automate logging and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See train.py\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch import callbacks\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class LightingWrapper(L.LightningModule):\n",
    "    \"\"\"Wraps a nn.Module with an associated loss function and optimizer.\"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, optim, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optim = optim\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optim(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # Text modality:\n",
    "        # x: (b, n) where each token is an integer in [0, n_classes) (vocab_size)\n",
    "        # y: (b, n) There n labels for n tokens (the predicted next token). Each \n",
    "        #    successive label gets a longer preceeding series of tokens to use as its\n",
    "        #    features due to masking in attention.\n",
    "        # Image modality:\n",
    "        # x: (b, n, d_feature) where each token is a flattened image patch\n",
    "        # y: (b,) There is a single class label for each image.\n",
    "        # The model handles both input shapes properly\n",
    "        logits = self.model(x)  # (b, n, n_classes) or (b, n_classes) depending on classification head\n",
    "        if isinstance(self.model.classifier, PerTokenClassifier):\n",
    "            logits = logits.view(-1, logits.shape[-1])  # (b*n, n_classes)\n",
    "        # This handles both output shapes\n",
    "        y = y.view(-1)  # (b*n) or (b)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Identical to training step\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        if isinstance(self.model.classifier, PerTokenClassifier):\n",
    "            logits = logits.view(-1, logits.shape[-1])\n",
    "        y = y.view(-1)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=None,\n",
    "    epochs=1,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss,\n",
    "    optim=torch.optim.AdamW,\n",
    "    lr=1e-3,\n",
    "    name=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a model, dataset, loss function and optimizer, train the model.\n",
    "    Uses Pytorch Lighting to handle logging, checkpointing, and GPU accelleration.\n",
    "    \"\"\"\n",
    "\n",
    "    wrapped_model = LightingWrapper(model, loss_fn, optim, lr)\n",
    "    ckpt = callbacks.ModelCheckpoint()\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        val_check_interval=1 / 4,\n",
    "        accelerator=\"auto\",\n",
    "        callbacks=[ckpt],\n",
    "        default_root_dir=\"logs/\" + name,\n",
    "    )\n",
    "\n",
    "    # Run the training loop\n",
    "    start_time = time.time()\n",
    "    trainer.fit(wrapped_model, train_loader, val_loader)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = str(timedelta(seconds=end_time - start_time))\n",
    "    print(f\"Training time: {elapsed_time}\")\n",
    "\n",
    "    return wrapped_model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See data.py\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def download_shakespear_data():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "    unique_chars = sorted(list(set(text)))\n",
    "    token_to_int = {ch: i for i, ch in enumerate(unique_chars)}\n",
    "    os.makedirs(\"data/shakespeare\", exist_ok=True)\n",
    "    with open(\"data/shakespeare/input.txt\", \"w\") as f:\n",
    "        f.write(text)\n",
    "    with open(\"data/shakespeare/vocab.json\", \"w\") as f:\n",
    "        json.dump(token_to_int, f)\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"create a mapping from unique vocab tokens to integers\"\"\"\n",
    "    # Get vocab data if not already downloaded\n",
    "    if not os.path.exists(\"data/input.txt\"):\n",
    "        download_shakespear_data()\n",
    "    with open(\"data/vocab.json\", \"r\") as f:\n",
    "        token_to_int = json.load(f)\n",
    "    encoded_text = [token_to_int[char] for char in text]\n",
    "    return torch.tensor(encoded_text, dtype=torch.long)\n",
    "\n",
    "\n",
    "def decode(tokens):\n",
    "    \"\"\"create a mapping from integers back to unique vocab tokens\"\"\"\n",
    "    # Get vocab data if not already downloaded\n",
    "    if not os.path.exists(\"data/shakespeare/input.txt\"):\n",
    "        download_shakespear_data()\n",
    "    with open(\"data/shakespeare/vocab.json\", \"r\") as f:\n",
    "        token_to_int = json.load(f)\n",
    "    int_to_token = {v: k for k, v in token_to_int.items()}\n",
    "    token_list = [int_to_token[token] for token in tokens]\n",
    "    return \"\".join(token_list)\n",
    "\n",
    "\n",
    "class ShakespeareDataset(data.Dataset):\n",
    "    def __init__(self, tokens_per_chunk):\n",
    "        super().__init__()\n",
    "        # Download data if not already downloaded\n",
    "        if not os.path.exists(\"data/shakespeare/input.txt\"):\n",
    "            download_shakespear_data()\n",
    "        with open(\"data/shakespeare/input.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        with open(\"data/shakespeare/vocab.json\", \"r\") as f:\n",
    "            token_to_int = json.load(f)\n",
    "        self.data = encode(text)\n",
    "        self.vocab_size = len(token_to_int)\n",
    "        self.block_size = tokens_per_chunk\n",
    "\n",
    "    def __len__(self):\n",
    "        # A single example of this text set is a chunk of characters with length block_size\n",
    "        return len(self.data) // self.block_size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # The corresponding label for each example is a chunk of characters of the same size,\n",
    "        # but shifted one character to the right. Thus the task is to predict the next character\n",
    "        # given all of the previous characters in a block. In this sense, the model learns to\n",
    "        # generate predictions based on with varying amounts of preceding characters, ranging\n",
    "        # from just a single character to the entire block.\n",
    "        x = self.data[i : i + self.block_size]\n",
    "        y = self.data[i + 1 : i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def get_data(dataset_name, batch_size, n_features=None, train_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Get dataloaders for a given dataset.\n",
    "    Returns:\n",
    "        train_loader: a DataLoader for the training set\n",
    "        val_loader: a DataLoader for the validation set\n",
    "        feature_dim: the dimensionality of each feature one example from the dataset\n",
    "        n_classes: the number of classes in the dataset\n",
    "    Feature_dim and n_classes are required for initializing the Transformer model.\n",
    "    \"\"\"\n",
    "    if dataset_name == \"shakespeare\":\n",
    "        n_features = n_features or 32\n",
    "        dataset = ShakespeareDataset(n_features)\n",
    "        n = len(dataset)\n",
    "        train_size = int(train_ratio * n)\n",
    "        test_size = n - train_size\n",
    "        train_data, val_data = data.random_split(dataset, [train_size, test_size])\n",
    "        train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = data.DataLoader(val_data, batch_size=batch_size)\n",
    "    \n",
    "    elif dataset_name == \"mnist\":\n",
    "        train_data = datasets.MNIST(root=\"data/mnist\", train=True, download=True, transform=transforms.ToTensor())\n",
    "        val_data = datasets.MNIST(root=\"data/mnist\", train=False, download=True, transform=transforms.ToTensor())\n",
    "        train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = data.DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' not supported. Try 'shakespeare' or 'mnist'.\")\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See run_image_example.py\n",
    "\n",
    "# Modality-specific parameters (images)\n",
    "patch_shape = (5, 5) # 6x6 grid of patches \n",
    "n_patches_per_image = 25\n",
    "# Based on 5x5 patches and 28x28 images, we can fit 25 patches per image.\n",
    "# The core transformer is agnostic of this number, but it learns a conv embedding based on the patch size.\n",
    "# So the patch shape at training must remain the same at inference, but different size images or different\n",
    "# conv strides can be used at inference time. Model is O(n^2) with this number.\n",
    "embedding = \"patch_conv\" # Learnable conv kernels that map patch pixels to d_model vectors.\n",
    "classifier = \"avg_pool\" # Average all n patch vectors into a single vector and do linear classification on that vector.\n",
    "\n",
    "# Model hyperparameters\n",
    "max_features = n_patches_per_image # Used for defining pos_encoding. Must be at least n_patches, but could be more.\n",
    "d_model = 15\n",
    "d_mlp = 24\n",
    "heads_per_block = 5\n",
    "num_blocks = 3\n",
    "dropout_rate = 0.0\n",
    "\n",
    "# Training parameters\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "loss_fn = F.cross_entropy\n",
    "optim = torch.optim.AdamW\n",
    "\n",
    "# FMM test\n",
    "fastmax = True\n",
    "p = 2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders\n",
    "    dataset_name = 'mnist'\n",
    "    train_loader, test_loader = get_data(dataset_name, batch_size)\n",
    "\n",
    "    # MNIST info\n",
    "    n_classes = 10\n",
    "    channels = 1\n",
    "\n",
    "    # Create model\n",
    "    model = Transformer(\n",
    "        out_dim=n_classes,\n",
    "        max_features=max_features,\n",
    "        d_model=d_model,\n",
    "        d_mlp=d_mlp,\n",
    "        heads_per_block=heads_per_block,\n",
    "        num_blocks=num_blocks,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_masking=False,\n",
    "        embedding=embedding,\n",
    "        classifier=classifier,\n",
    "        patch_shape=patch_shape,\n",
    "        patch_channels=channels,\n",
    "        fastmax=fastmax,\n",
    "        fmm_p=p\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    model = train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        epochs=epochs,\n",
    "        loss_fn=loss_fn,\n",
    "        optim=optim,\n",
    "        lr=lr,\n",
    "        name=dataset_name,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
